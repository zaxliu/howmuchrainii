{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "#\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# \n",
    "from sklearn.preprocessing import StandardScaler, Imputer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cross_validation import cross_val_score, ShuffleSplit\n",
    "from sklearn.learning_curve import learning_curve, validation_curve\n",
    "from sklearn.metrics import make_scorer\n",
    "#\n",
    "import xgboost as xgb\n",
    "#\n",
    "from blending import BlendedRegressor\n",
    "from cleaning import TargetThresholdFilter, LogPlusOne\n",
    "#\n",
    "pd.set_option('display.max_columns', 500)  # force pandas to display all columns for better visual inspection\n",
    "# plot plots inline\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# trn = pd.read_csv('../data/train.csv')\n",
    "# trn = pd.read_csv('../data/train_10.csv', index_col=0)  # column #0 in our file is DataFrame index\n",
    "trn = pd.read_csv('../data/train_1.csv', index_col=0)\n",
    "# test = pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0.0175 secs\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "trn_withRef = trn[trn['Ref'].notnull()]\n",
    "# test_withRef = test[test['Ref'].notnull()]\n",
    "del trn\n",
    "print 'Time elapsed: {:.4f} secs'.format(time.time()-t)  # toc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0.4887 secs\n"
     ]
    }
   ],
   "source": [
    "# Combine observations by 'Id', aggregate features\n",
    "t = time.time()\n",
    "trn_withRef_comb = trn_withRef.groupby('Id').agg(['mean','std','median','count','min', 'max'])\n",
    "trn_withRef_comb.columns = ['_'.join(tup) for (i,tup) in enumerate(trn_withRef_comb.columns.values)]\n",
    "trn_withRef_comb = trn_withRef_comb.drop(['Expected_count', 'Expected_median', 'Expected_std', 'Expected_min','Expected_max'], axis =1)\n",
    "# test_withRef_comb = test_withRef.groupby('Id').agg(['mean','std','median','count','min', 'max'])\n",
    "# test_withRef_comb.columns = ['_'.join(tup) for (i,tup) in enumerate(test_withRef_comb.columns.values)]\n",
    "# test_withRef_comb = test_withRef_comb.drop(['Expected_count', 'Expected_median', 'Expected_std', 'Expected_min','Expected_max'], axis =1)\n",
    "del trn_withRef\n",
    "print 'Time elapsed: {:.4f} secs'.format(time.time()-t)  # toc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = trn_withRef_comb.loc[:, 'minutes_past_mean':'Kdp_5x5_90th_max']  # NOTE: used range slicing on column\n",
    "y1 = np.log10(1+trn_withRef_comb['Expected_mean'])\n",
    "y2 = trn_withRef_comb['Expected_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "lpo = LogPlusOne()\n",
    "imp = Imputer(strategy='median')  # Get a imputor with column-mean filling config\n",
    "ss = StandardScaler(copy=False, with_mean=True, with_std=True)\n",
    "clf = RandomForestRegressor(n_estimators=10, max_features='sqrt', max_depth=5, n_jobs=4, random_state=0)\n",
    "pip1 = Pipeline([('imp',imp), ('ss',ss), ('clf', clf)]) \n",
    "pip2 = Pipeline([('lpo',lpo), ('imp',imp), ('ss',ss), ('clf', clf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MAE_logy(ground_truth, predictions):\n",
    "    \"\"\"Custom scoring function for log(y) or log(1+y)\n",
    "       NOTE: please change this if you use another non-linearity on y\n",
    "    \"\"\"\n",
    "    return np.float64(np.mean(np.abs(np.power(10,ground_truth) - np.power(10,predictions))))\n",
    "def LogPlusOne_score(ground_truth, predictions, lpo):\n",
    "    return np.float64(np.mean(np.abs(ground_truth - (np.power(10, predictions) - 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.3314446451 2.44936404358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  10 out of  10 | elapsed:   11.1s finished\n"
     ]
    }
   ],
   "source": [
    "scorer = make_scorer(MAE_logy, greater_is_better=True)  # define scoring metric\n",
    "scores = cross_val_score(estimator=pip1, X=X, y=y1, scoring=scorer, cv=10, n_jobs=2, verbose=1)\n",
    "print np.mean(scores), np.std(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.3314446451 2.44936404358\n",
      "aha\n",
      "aha\n",
      "ahaaha\n",
      "\n",
      "ahaaha\n",
      "\n",
      "ahaaha\n",
      "\n",
      "ahaaha\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  10 out of  10 | elapsed:   11.1s finished\n"
     ]
    }
   ],
   "source": [
    "scorer = make_scorer(LogPlusOne_score, greater_is_better=True)  # define scoring metric\n",
    "scores = cross_val_score(estimator=pip2, X=X, y=y2, scoring=scorer, cv=10, n_jobs=2, verbose=1)\n",
    "print np.mean(scores), np.std(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.125\n",
      "0.125\n"
     ]
    }
   ],
   "source": [
    "y1 = pd.Series(np.array([1, 2, 3, 4]))\n",
    "y2 = pd.Series(np.array([1.1, 1.9, 3.2, 3.9]))\n",
    "print MAE_logy(np.log10(1+y1), np.log10(1+y2))\n",
    "print lpo.metric(y1, np.log10(1+y2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

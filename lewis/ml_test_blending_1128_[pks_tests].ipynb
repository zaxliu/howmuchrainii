{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Basics\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Sklearn\n",
    "from sklearn.preprocessing import StandardScaler, Imputer \n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.cross_validation import cross_val_score, ShuffleSplit\n",
    "from sklearn.learning_curve import learning_curve, validation_curve\n",
    "from sklearn.metrics import make_scorer\n",
    "# xgboost\n",
    "import xgboost as xgb\n",
    "# Our custom modules\n",
    "sys.path.append('..')\n",
    "from anrg.pipeline import Pipeline, SelectTailK, LeaveTailK, DummyRegressor\n",
    "from anrg.blending import BlendedRegressor\n",
    "from anrg.cleaning import TargetThresholdFilter, LogPlusOne\n",
    "from anrg.mp import MPRegressor\n",
    "##### setting #######\n",
    "pd.set_option('display.max_columns', 500)  # force pandas to display all columns for better visual inspection\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trn = pd.read_csv('../data/train.csv')\n",
    "# trn = pd.read_csv('../data/train_10.csv', index_col=0)  # column #0 in our file is DataFrame index\n",
    "# trn = pd.read_csv('../data/train_1.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 15.4321 secs\n"
     ]
    }
   ],
   "source": [
    "# Combine observations by 'Id', aggregate features\n",
    "t = time.time()\n",
    "trn_comb = trn.groupby('Id').agg(['mean','std','median','count', 'sem', 'min', 'max'])\n",
    "trn_comb.columns = ['_'.join(tup) for (i,tup) in enumerate(trn_comb.columns.values)]\n",
    "trn_comb = trn_comb.drop(['Expected_count', 'Expected_median', 'Expected_std', 'Expected_sem', 'Expected_min','Expected_max'], axis =1)\n",
    "\n",
    "# ignore id's where all Ref vales are NaN\n",
    "trn_withRef_comb = trn_comb[pd.notnull(trn_comb.Ref_mean)]\n",
    "\n",
    "print 'Time elapsed: {:.4f} secs'.format(time.time()-t)  # toc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(731556, 155) (731556,)\n"
     ]
    }
   ],
   "source": [
    "y = trn_withRef_comb['Expected_mean']\n",
    "trn_withRef_comb = trn_withRef_comb.drop(['Expected_mean'], axis =1)\n",
    "\n",
    "trn_withRef_comb['MP'] = np.log10(1+pow(pow(10, trn_withRef_comb['Ref_mean']/10)/200, 0.625 * 0.82))\n",
    "X = trn_withRef_comb.loc[:, 'minutes_past_mean':'MP']  # NOTE: used range slicing on column\n",
    "\n",
    "print X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ttf = TargetThresholdFilter(threshold=45)\n",
    "lpo = LogPlusOne()\n",
    "imp = Imputer(strategy='median', copy=False)  # Get a imputor with column-mean filling config\n",
    "ss = StandardScaler(copy=False, with_mean=True, with_std=True)\n",
    "base1 = Pipeline([('sel', LeaveTailK(K=1)), ('rf', RandomForestRegressor(n_estimators=500, max_features=0.2, max_depth=25, n_jobs=7))])  # NOTE: n_jobs=-1 will use all of your cores, set to a prefered number e.g. 4\n",
    "base2 = Pipeline([('sel', LeaveTailK(K=1)), ('xgb', xgb.sklearn.XGBRegressor(n_estimators=500, nthread=6))], copy = False)\n",
    "base3 = Pipeline([('sel', LeaveTailK(K=1)), ('ri', Ridge(alpha=2.0))], copy = False)\n",
    "base4 = Pipeline([('sel', LeaveTailK(K=1)), ('la', Lasso(alpha=0.01))], copy = False)\n",
    "base5 = Pipeline([('sel', SelectTailK(K=1)), ('mp', DummyRegressor())], copy = False)\n",
    "blender = Ridge(alpha=2)\n",
    "clf = BlendedRegressor(base_models=(base1, base2, base3, base4, base5), blending_model=blender, blending_split=0.2, with_feature=True)\n",
    "pip = Pipeline([('ttf',ttf), ('lpo',lpo), ('imp',imp), ('ss',ss), ('clf',clf)], copy=True)  # a Pipeline wrapper to chain'em up\n",
    "def LogPlusOne_score(ground_truth, predictions, lpo=lpo):\n",
    "    return np.float64(np.mean(np.abs(ground_truth - (np.power(10, predictions) - 1))))\n",
    "scorer = make_scorer(LogPlusOne_score, greater_is_better=False)  # define scoring metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = cross_val_score(estimator=pip, X=X, y=y, scoring=scorer, cv=10, n_jobs=1, verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ttf = TargetThresholdFilter(threshold=45)\n",
    "# base1 = RandomForestRegressor(n_estimators=500, max_features=0.2, max_depth=25, n_jobs=7)  # NOTE: n_jobs=-1 will use all of your cores, set to a prefered number e.g. 4\n",
    "# base2 = xgb.sklearn.XGBRegressor(n_estimators=500, nthread=6) # base3 = Ridge(alpha=2.0)# base4 = Lasso(alpha=0.01) # base5: MP\n",
    "# blender = Ridge(1)\n",
    "# clf = BlendedRegressor(base_models=(base1, base2, base3, base4, base5), blending_model=blender, blending_split=0.1)\n",
    "# pip = Pipeline([('ttf',ttf), ('lpo',lpo), ('imp',imp), ('ss',ss), ('clf',clf)])  # a Pipeline wrapper to chain'em up\n",
    "print np.mean(scores), np.std(scores)\n",
    "print pip.get_params()['steps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ttf = TargetThresholdFilter(threshold=45)\n",
    "# base1 = RandomForestRegressor(n_estimators=500, max_features=0.2, max_depth=25, n_jobs=7)  # NOTE: n_jobs=-1 will use all of your cores, set to a prefered number e.g. 4\n",
    "# base2 = xgb.sklearn.XGBRegressor(n_estimators=500, nthread=6) # base3 = Ridge(alpha=2.0)# base4 = Lasso(alpha=0.01) # base5: MP\n",
    "# blender = Ridge(2)\n",
    "# clf = BlendedRegressor(base_models=(base1, base2, base3, base4, base5), blending_model=blender, blending_split=0.2)\n",
    "# pip = Pipeline([('ttf',ttf), ('lpo',lpo), ('imp',imp), ('ss',ss), ('clf',clf)])  # a Pipeline wrapper to chain'em up\n",
    "print np.mean(scores), np.std(scores)\n",
    "print pip.get_params()['steps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sweep parameter\n",
    "param_name, param_range='clf__blending_model__alpha', np.arange(1.5, 2.5, 0.25)\n",
    "train_scores, test_scores= validation_curve(estimator=pip, X=X, y=y, scoring=scorer, cv=10, n_jobs=1, \n",
    "                          param_name=param_name, param_range=param_range, verbose=3)\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)# Sweep parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plotting the validation curver, i.e. trn/val error versus parameter value\n",
    "# Sweep alpha = [0.5, 0.75, 1, 1.25, 1.5]\n",
    "# 100% data\n",
    "print pip.get_params()['steps']\n",
    "print param_range\n",
    "print train_scores_mean\n",
    "print test_scores_mean\n",
    "plt.title(\"Validation Curve\")\n",
    "plt.xlabel(param_name)\n",
    "plt.ylabel(\"Score\")\n",
    "plt.plot(param_range, train_scores_mean, label=\"Training score\", color=\"r\")\n",
    "# plt.fill_between(param_range, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.2, color=\"r\")\n",
    "plt.twinx()\n",
    "plt.plot(param_range, test_scores_mean, label=\"Cross-validation score\", color=\"g\")\n",
    "# plt.fill_between(param_range, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.2, color=\"g\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plotting the validation curver, i.e. trn/val error versus parameter value\n",
    "# Sweep alpha = np.arange(1.5, 2.5, 0.25)\n",
    "# 100% data\n",
    "print pip.get_params()['steps']\n",
    "print param_range\n",
    "print train_scores_mean\n",
    "print test_scores_mean\n",
    "plt.title(\"Validation Curve\")\n",
    "plt.xlabel(param_name)\n",
    "plt.ylabel(\"Score\")\n",
    "plt.plot(param_range, train_scores_mean, label=\"Training score\", color=\"r\")\n",
    "# plt.fill_between(param_range, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.2, color=\"r\")\n",
    "plt.twinx()\n",
    "plt.plot(param_range, test_scores_mean, label=\"Cross-validation score\", color=\"g\")\n",
    "# plt.fill_between(param_range, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.2, color=\"g\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sweep parameter\n",
    "param_name, param_range='clf__blending_split', [0.025, 0.05, 0.075, 0.1, 0.125, 0.15, 0.175]\n",
    "train_scores, test_scores= validation_curve(estimator=pip, X=X, y=y, scoring=scorer, cv=10, n_jobs=1, \n",
    "                          param_name=param_name, param_range=param_range, verbose=3)\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)# Sweep parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plotting the validation curver, i.e. trn/val error versus parameter value\n",
    "# 'clf__blending_split', [0.025, 0.05, 0.075, 0.1, 0.125, 0.15, 0.175]\n",
    "# 100% data\n",
    "print pip.get_params()['steps']\n",
    "print param_range\n",
    "print train_scores_mean\n",
    "print test_scores_mean\n",
    "plt.title(\"Validation Curve\")\n",
    "plt.xlabel(param_name)\n",
    "plt.ylabel(\"Score\")\n",
    "plt.plot(param_range, train_scores_mean, label=\"Training score\", color=\"r\")\n",
    "# plt.fill_between(param_range, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.2, color=\"r\")\n",
    "plt.twinx()\n",
    "plt.plot(param_range, test_scores_mean, label=\"Cross-validation score\", color=\"g\")\n",
    "# plt.fill_between(param_range, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.2, color=\"g\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(copy=True,\n",
       "     steps=[('ttf', TargetThresholdFilter(threshold=45)), ('lpo', LogPlusOne()), ('imp', Imputer(axis=0, copy=False, missing_values='NaN', strategy='median',\n",
       "    verbose=0)), ('ss', StandardScaler(copy=False, with_mean=True, with_std=True)), ('clf', BlendedRegressor(base_models=(Pipeline(copy=True,\n",
       "     ...lse, random_state=None, solver='auto', tol=0.001),\n",
       "         blending_split=0.2, with_feature=True))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pip.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('../data/test.csv')\n",
    "test_comb = test.groupby('Id').agg(['mean','std','median','count', 'sem', 'min', 'max'])\n",
    "test_comb.columns = ['_'.join(tup) for (i,tup) in enumerate(test_comb.columns.values)]\n",
    "test_withRef_comb = test_comb[pd.notnull(test_comb.Ref_mean)]\n",
    "test_withRef_comb['MP'] = np.log10(1+pow(pow(10, test_withRef_comb['Ref_mean']/10)/200, 0.625 * 0.82))\n",
    "\n",
    "test_X = test_withRef_comb.loc[:, 'minutes_past_mean':'MP']   # NOTE: used range slicing on column\n",
    "test_y_predict = 10**pip.predict(X=test_X)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/result_20151128_094242.csv\n"
     ]
    }
   ],
   "source": [
    "# Ref-samples\n",
    "test_result_withRef = pd.DataFrame()\n",
    "test_result_withRef['Id'] = test_withRef_comb.index\n",
    "test_result_withRef['Expected'] = test_y_predict\n",
    "# All-samples\n",
    "test_result = pd.DataFrame()\n",
    "test_result['Id'] = test['Id'].unique()\n",
    "# Merge and set Non-Ref samples to -1\n",
    "test_result = pd.merge(test_result, test_result_withRef, how='left', on=['Id'], sort=True)\n",
    "test_result.loc[test_result['Expected'].isnull(), 'Expected'] = -1\n",
    "# Write file\n",
    "datetime_str = time.strftime('%Y%m%d_%H%M%S')\n",
    "test_result.to_csv('../data/result_'+datetime_str+'.csv', index=False)\n",
    "print '../data/result_'+datetime_str+'.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

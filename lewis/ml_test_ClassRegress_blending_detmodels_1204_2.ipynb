{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Basics\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Sklearn\n",
    "from sklearn.preprocessing import StandardScaler, Imputer \n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.cross_validation import cross_val_score, ShuffleSplit, train_test_split\n",
    "from sklearn.learning_curve import learning_curve, validation_curve\n",
    "from sklearn.metrics import make_scorer, confusion_matrix\n",
    "# xgboost\n",
    "import xgboost as xgb\n",
    "# Our custom modules\n",
    "sys.path.append('..')\n",
    "from anrg.pipeline import Pipeline, SelectTailK, LeaveTailK, SelectK2Last, DummyRegressor\n",
    "from anrg.blending import BlendedRegressor\n",
    "from anrg.cleaning import TargetThresholdFilter, LogPlusOne\n",
    "from anrg.classified_regression import ClassifiedRegressor\n",
    "##### setting #######\n",
    "pd.set_option('display.max_columns', 500)  # force pandas to display all columns for better visual inspection\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn = pd.read_csv('../data/train.csv')\n",
    "# trn = pd.read_csv('../data/train_10.csv', index_col=0)  # column #0 in our file is DataFrame index\n",
    "# trn = pd.read_csv('../data/train_1.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 11.9503 secs\n"
     ]
    }
   ],
   "source": [
    "# Combine observations by 'Id', aggregate features\n",
    "t = time.time()\n",
    "trn_comb = trn.groupby('Id').agg(['mean','std','median','count', 'min', 'max'])\n",
    "trn_comb.columns = ['_'.join(tup) for (i,tup) in enumerate(trn_comb.columns.values)]\n",
    "# ignore id's where all Ref vales are NaN\n",
    "trn_withRef_comb = trn_comb[pd.notnull(trn_comb.Ref_mean)]\n",
    "# Gargage collection\n",
    "del trn\n",
    "del trn_comb\n",
    "# Timing\n",
    "print 'Time elapsed: {:.4f} secs'.format(time.time()-t)  # toc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add deterministic estimator as columns\n",
    "# MP params=0.82\n",
    "trn_withRef_comb.loc[:,'MP'] = 1+pow(pow(10, trn_withRef_comb['Ref_mean']/10)/200, 0.625 * 0.82)\n",
    "#KDP\n",
    "kdp_aa, kdp_bb, kdp_aa_scaling, kdp_bb_scaling = 4.06, 0.0866, 0.33, 0.79\n",
    "trn_withRef_comb.loc[:,'KDP'] = np.sign(trn_withRef_comb['Kdp_mean'])*(kdp_aa*kdp_aa_scaling)*pow(np.abs(trn_withRef_comb['Kdp_mean']),kdp_bb*kdp_bb_scaling)\n",
    "#KDP_ZDR\n",
    "kdpzdr_aa, kdpzdr_bb, kdpzdr_cc, kdpzdr_aa_scaling = 13.6, 0.0968, -0.286, 0.003\n",
    "trn_withRef_comb.loc[:, 'KDP_ZDR'] = np.sign(trn_withRef_comb['Kdp_mean'])*(kdpzdr_aa*kdpzdr_aa_scaling)*pow(np.abs(trn_withRef_comb['Kdp_mean']),kdpzdr_bb)*pow(pow(10,trn_withRef_comb['Zdr_mean']/10),kdpzdr_cc)\n",
    "#REF_ZDR\n",
    "refzdr_aa, refzdr_bb, refzdr_cc, refzdr_aa_scaling, refzdr_bb_scaling, refzdr_cc_scaling = 0.00746, 0.945, -4.76, 0.0017, 0.9, 0.8\n",
    "trn_withRef_comb.loc[:,'REF_ZDR'] = (refzdr_aa*refzdr_aa_scaling)*pow(pow(10,trn_withRef_comb['Ref_mean']/10),refzdr_bb*refzdr_bb_scaling)*pow(pow(10,trn_withRef_comb['Zdr_mean']/10),refzdr_cc*refzdr_cc_scaling)\n",
    "# Regularizing negative predictions to 0\n",
    "for name in ['MP','KDP', 'KDP_ZDR', 'REF_ZDR']:\n",
    "    trn_withRef_comb.loc[trn_withRef_comb[name]<0, name] = 0\n",
    "# Taking log(1+x) on all predictions\n",
    "trn_withRef_comb.loc[:, ['MP','KDP', 'KDP_ZDR', 'REF_ZDR']] = np.log10(1+trn_withRef_comb.loc[:, ['MP','KDP', 'KDP_ZDR', 'REF_ZDR']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(731556, 136) (731556,)\n"
     ]
    }
   ],
   "source": [
    "# Extract X and y\n",
    "y = trn_withRef_comb['Expected_mean']\n",
    "X = trn_withRef_comb.ix[:, [col for col in trn_withRef_comb.columns if not 'Expected' in col]]  # NOTE: used range slicing on column\n",
    "# Garbage collection\n",
    "del trn_withRef_comb\n",
    "print X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "seed = 12345\n",
    "n_tree = 500\n",
    "n_jobs = 7\n",
    "#\n",
    "lpo = LogPlusOne()\n",
    "imp = Imputer(strategy='median', copy=False)  # Get a imputor with column-mean filling config\n",
    "ss = StandardScaler(copy=False, with_mean=True, with_std=True)\n",
    "def LogPlusOne_score(ground_truth, predictions):\n",
    "    return np.float64(np.mean(np.abs(ground_truth - (np.power(10, predictions) - 1))))\n",
    "scorer = make_scorer(LogPlusOne_score, greater_is_better=False)  # define scoring metric\n",
    "reg_sub = [None, None]\n",
    "for i in [0, 1]:\n",
    "    base1 = Pipeline([('sel', LeaveTailK(K=4)), ('rf', RandomForestRegressor(n_estimators=n_tree, max_features=0.2, max_depth=25, n_jobs=n_jobs , random_state=seed))], copy = False)\n",
    "    base2 = Pipeline([('sel', LeaveTailK(K=4)), ('xgb', xgb.sklearn.XGBRegressor(n_estimators=n_tree, nthread=n_jobs , seed=seed))], copy = False)\n",
    "    base3 = Pipeline([('sel', LeaveTailK(K=4)), ('ri', Ridge(alpha=2.0, random_state=seed))], copy = False)\n",
    "    base4 = Pipeline([('sel', LeaveTailK(K=4)), ('la', Lasso(alpha=0.01, random_state=seed))], copy = False)\n",
    "    base5 = Pipeline([('sel', SelectK2Last(K=4)), ('mp', DummyRegressor())], copy = False)\n",
    "    base6 = Pipeline([('sel', SelectK2Last(K=3)), ('kdp', DummyRegressor())], copy = False)\n",
    "    base7 = Pipeline([('sel', SelectK2Last(K=2)), ('kdp_zdr', DummyRegressor())], copy = False)\n",
    "    base8 = Pipeline([('sel', SelectK2Last(K=1)), ('ref_zdr', DummyRegressor())], copy = False)\n",
    "    blender = Ridge(alpha=1.45, random_state=seed)\n",
    "    base_models=(base1, base2, base3, base4, base5, base6, base7, base8)\n",
    "    reg_sub[i] = BlendedRegressor(base_models=base_models, blending_model=blender, blending_split=0.15, with_feature=True, random_state=seed)\n",
    "reg_sub = tuple(reg_sub)\n",
    "clf = xgb.sklearn.XGBClassifier(n_estimators=n_tree, nthread=n_jobs , seed=seed)\n",
    "reg = ClassifiedRegressor(labeling_thresh=np.log10(1+45) , classifier=clf, proba_thresh=0.6, regressors=reg_sub, verbose=1)\n",
    "pip = Pipeline([('lpo',lpo), ('imp',imp), ('ss',ss), ('reg',reg)], copy=True)  # a Pipeline wrapper to chain'em up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] no parameters to be set .........................................\n",
      "Training classifier, total samples = 658400... label False subtotal 640131... label True subtotal 18269... fitting regressors, total samples = 658400... regressor 0 fits label False subtotal 640131... regressor 1 fits label True subtotal 18269...  \n",
      "Regressor prediction, total samples = 73156... regressor 0 predicts label False subtotal 73156... regressor 1 predicts label True subtotal 0...  \n",
      "[CV] .............. no parameters to be set, score=-22.273793 -  54.8s\n",
      "[CV] no parameters to be set .........................................\n",
      "Training classifier, total samples = 658400... label False subtotal 640143... label True subtotal 18257... fitting regressors, total samples = 658400... regressor 0 fits label False subtotal 640143... regressor 1 fits label True subtotal 18257...  \n",
      "Regressor prediction, total samples = 73156... regressor 0 predicts label False subtotal 73156... regressor 1 predicts label True subtotal 0...  \n",
      "[CV] .............. no parameters to be set, score=-22.544937 -  54.8s\n",
      "[CV] no parameters to be set .........................................\n",
      "Training classifier, total samples = 658400... label False subtotal 640250... label True subtotal 18150... fitting regressors, total samples = 658400... regressor 0 fits label False subtotal 640250... regressor 1 fits label True subtotal 18150...  \n",
      "Regressor prediction, total samples = 73156... regressor 0 predicts label False subtotal 73155... regressor 1 predicts label True subtotal 1...  \n",
      "[CV] .............. no parameters to be set, score=-23.426886 -  54.6s\n",
      "[CV] no parameters to be set .........................................\n",
      "Training classifier, total samples = 658400... label False subtotal 640278... label True subtotal 18122... fitting regressors, total samples = 658400... regressor 0 fits label False subtotal 640278... regressor 1 fits label True subtotal 18122...  \n",
      "Regressor prediction, total samples = 73156... regressor 0 predicts label False subtotal 73156... regressor 1 predicts label True subtotal 0...  \n",
      "[CV] .............. no parameters to be set, score=-24.938773 -  54.6s\n",
      "[CV] no parameters to be set .........................................\n",
      "Training classifier, total samples = 658400... label False subtotal 640227... label True subtotal 18173... fitting regressors, total samples = 658400... regressor 0 fits label False subtotal 640227... regressor 1 fits label True subtotal 18173...  \n",
      "Regressor prediction, total samples = 73156... regressor 0 predicts label False subtotal 73156... regressor 1 predicts label True subtotal 0...  \n",
      "[CV] .............. no parameters to be set, score=-23.780573 -  54.7s\n",
      "[CV] no parameters to be set .........................................\n",
      "Training classifier, total samples = 658400... label False subtotal 640678... label True subtotal 17722... fitting regressors, total samples = 658400... regressor 0 fits label False subtotal 640678... regressor 1 fits label True subtotal 17722...  \n",
      "Regressor prediction, total samples = 73156... regressor 0 predicts label False subtotal 73156... regressor 1 predicts label True subtotal 0...  \n",
      "[CV] .............. no parameters to be set, score=-23.942321 -  53.7s\n",
      "[CV] no parameters to be set .........................................\n",
      "Training classifier, total samples = 658401... label False subtotal 640001... label True subtotal 18400... fitting regressors, total samples = 658401... regressor 0 fits label False subtotal 640001... regressor 1 fits label True subtotal 18400...  \n",
      "Regressor prediction, total samples = 73155... regressor 0 predicts label False subtotal 73155... regressor 1 predicts label True subtotal 0...  \n",
      "[CV] .............. no parameters to be set, score=-19.930151 -  57.0s\n",
      "[CV] no parameters to be set .........................................\n",
      "Training classifier, total samples = 658401... label False subtotal 640244... label True subtotal 18157... fitting regressors, total samples = 658401... regressor 0 fits label False subtotal 640244... regressor 1 fits label True subtotal 18157...  \n",
      "Regressor prediction, total samples = 73155... regressor 0 predicts label False subtotal 73155... regressor 1 predicts label True subtotal 0...  \n",
      "[CV] .............. no parameters to be set, score=-25.623868 -  58.2s\n",
      "[CV] no parameters to be set .........................................\n",
      "Training classifier, total samples = 658401... label False subtotal 640315... label True subtotal 18086... fitting regressors, total samples = 658401... regressor 0 fits label False subtotal 640315... regressor 1 fits label True subtotal 18086...  \n",
      "Regressor prediction, total samples = 73155... regressor 0 predicts label False subtotal 73155... regressor 1 predicts label True subtotal 0...  \n",
      "[CV] .............. no parameters to be set, score=-23.691809 -  54.5s\n",
      "[CV] no parameters to be set .........................................\n",
      "Training classifier, total samples = 658401... label False subtotal 640342... label True subtotal 18059... fitting regressors, total samples = 658401... regressor 0 fits label False subtotal 640342... regressor 1 fits label True subtotal 18059...  \n",
      "Regressor prediction, total samples = 73155... regressor 0 predicts label False subtotal 73155... regressor 1 predicts label True subtotal 0...  \n",
      "[CV] .............. no parameters to be set, score=-21.476099 -  54.1s\n",
      "-23.1629209816 1.5816728386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  9.2min finished\n"
     ]
    }
   ],
   "source": [
    "# label at 45, class xgb 500, proba_thresh=0\n",
    "scores = cross_val_score(estimator=pip, X=X, y=y, scoring=scorer, cv=10, n_jobs=1, verbose=3)\n",
    "print np.mean(scores), np.std(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
